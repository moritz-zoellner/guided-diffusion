#!/bin/sh
#SBATCH -J rollout_baseDP_multi
#SBATCH -A rpaleja
#SBATCH -p a100-40gb
#SBATCH --cpus-per-task=8
#SBATCH --gpus-per-node=1
#SBATCH --mem=32G
#SBATCH --qos=normal
#SBATCH -t 24:00:00
#SBATCH -o slurm-%j.out

module load conda
#conda activate /scratch/gilbreth/zoellner/conda/envs/guided_diffusion
conda activate guided_diffusion

export MUJOCO_GL=egl
export EGL_DEVICE_ID=0
export CUDA_VISIBLE_DEVICES=0
unset DISPLAY

echo "Starting base rollout training via SLURM"
cd ~/src/guided-diffusion

RUN_DIR=/scratch/gilbreth/zoellner/diffusion_runs/run_20260122_211123/20260122211129/models
OUT_DIR=/scratch/gilbreth/zoellner/diffusion_rollouts/sr_rollout

for EPOCH in 1300 1500 1700 1850 2000; do
  CKPT_PATH=${RUN_DIR}/model_epoch_${EPOCH}.pth
  echo "Running rollout for epoch ${EPOCH}: ${CKPT_PATH}"
  python -u experiments/rollout_base_policy.py \
    --n_rollouts 50 \
    --output_path ${OUT_DIR} \
    --ckpt_path ${CKPT_PATH} \
    --n_step_rollout_video 1 \
    --horizon 500 \
    --name model_epoch_${EPOCH}

done

echo "Finished base rollout?"
